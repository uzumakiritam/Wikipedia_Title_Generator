{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[]},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11378511,"sourceType":"datasetVersion","datasetId":7124096},{"sourceId":11391962,"sourceType":"datasetVersion","datasetId":7134363}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport re\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nimport numpy as np\nimport random\nimport time\nfrom collections import Counter\nimport spacy\nfrom tqdm import tqdm\nimport os\nimport gc\n\n# Download necessary NLTK resources\nnltk.download('punkt')\nnltk.download('stopwords')\nnltk.download('wordnet')\nnltk.download('punkt_tab')","metadata":{"id":"NYX8c9DS-hGw","trusted":true,"execution":{"iopub.status.busy":"2025-04-15T17:49:11.304333Z","iopub.execute_input":"2025-04-15T17:49:11.304569Z","iopub.status.idle":"2025-04-15T17:49:25.194970Z","shell.execute_reply.started":"2025-04-15T17:49:11.304541Z","shell.execute_reply":"2025-04-15T17:49:25.194356Z"}},"outputs":[{"name":"stderr","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n[nltk_data] Downloading package punkt_tab to /usr/share/nltk_data...\n[nltk_data]   Package punkt_tab is already up-to-date!\n","output_type":"stream"},{"execution_count":1,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":1},{"cell_type":"code","source":"SEED = 42\ntorch.manual_seed(SEED)\ntorch.cuda.manual_seed(SEED) if torch.cuda.is_available() else None\nnp.random.seed(SEED)\nrandom.seed(SEED)","metadata":{"id":"ADeg53h5-0NI","trusted":true,"execution":{"iopub.status.busy":"2025-04-15T17:49:25.196359Z","iopub.execute_input":"2025-04-15T17:49:25.196722Z","iopub.status.idle":"2025-04-15T17:49:25.205295Z","shell.execute_reply.started":"2025-04-15T17:49:25.196704Z","shell.execute_reply":"2025-04-15T17:49:25.204670Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"\n# Load spaCy for sentence tokenization\ntry:\n    nlp = spacy.load(\"en_core_web_sm\")\nexcept:\n    import subprocess\n    subprocess.call(\"python -m spacy download en_core_web_sm\", shell=True)\n    nlp = spacy.load(\"en_core_web_sm\")\n\n# Initialize NLP tools\nstop_words = set(stopwords.words('english'))\nminimal_stopwords = set(['the', 'and', 'a', 'of', 'to', 'in', 'that', 'it', 'with', 'for', 'on', 'at'])\nlemmatizer = WordNetLemmatizer()\n\ndef improved_preprocess(text, lower_case=True, lemmatize=True, stopword_removal=True):\n    \"\"\"Improved preprocessing function that preserves more contextual information\"\"\"\n    if lower_case:\n        text = text.lower()\n\n    # Remove non-alphanumeric characters except for basic punctuation\n    text = re.sub(r'[^a-zA-Z0-9\\s\\.\\,\\?\\!]', '', text)\n\n    # Tokenize\n    tokens = nltk.word_tokenize(text)\n\n    # Remove stopwords if specified\n    if stopword_removal:\n        tokens = [word for word in tokens if word not in minimal_stopwords]\n\n    # Lemmatize if specified\n    if lemmatize:\n        tokens = [lemmatizer.lemmatize(word) for word in tokens]\n\n    # Join tokens back into text\n    processed_text = \" \".join(tokens)\n    return processed_text\n\ndef tokenize(text):\n    \"\"\"Tokenize text to words\"\"\"\n    return nltk.word_tokenize(text.lower())\n\ndef build_vocab(sentences, min_freq_ratio=0.01):\n    \"\"\"Build vocabulary from sentences with minimum frequency threshold\"\"\"\n    all_tokens = [token for sent in sentences for token in tokenize(sent)]\n    total = len(all_tokens)\n    counter = Counter(all_tokens)\n\n    # Calculate minimum count threshold\n    min_count = max(1, int(total * min_freq_ratio))\n\n    # Initialize vocabulary with special tokens\n    vocab = {\"<pad>\": 0, \"<unk>\": 1, \"<bos>\": 2, \"<eos>\": 3}\n    idx = 4\n\n    # Add words that meet the frequency threshold\n    for word, count in counter.items():\n        if count >= min_count:\n            vocab[word] = idx\n            idx += 1\n\n    print(f\"Vocabulary size: {len(vocab)}\")\n    print(f\"Min count threshold: {min_count}\")\n    return vocab\n\nclass WikiTitleDataset(Dataset):\n    def __init__(self, df, vocab, max_length_text=512, max_length_title=30):\n        self.df = df\n        self.vocab = vocab\n        self.max_length_text = max_length_text\n        self.max_length_title = max_length_title\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        text = self.df.iloc[idx]['text']\n        title = self.df.iloc[idx]['title']\n\n        # Convert text to indices\n        text_tokens = tokenize(text)[:self.max_length_text]\n        text_indices = [self.vocab.get(token, self.vocab['<unk>']) for token in text_tokens]\n\n        # Convert title to indices\n        title_tokens = tokenize(title)[:self.max_length_title-2]  # -2 for <bos> and <eos>\n        title_indices = [self.vocab['<bos>']] + [self.vocab.get(token, self.vocab['<unk>']) for token in title_tokens] + [self.vocab['<eos>']]\n\n        return {\n            'text': torch.tensor(text_indices, dtype=torch.long),\n            'title': torch.tensor(title_indices, dtype=torch.long),\n            'raw_text': text,\n            'raw_title': title\n        }\n\ndef collate_fn(batch):\n    \"\"\"Custom collate function for DataLoader\"\"\"\n    # Sort batch by text length in descending order for packed sequences\n    batch = sorted(batch, key=lambda x: len(x['text']), reverse=True)\n\n    text_lengths = [len(item['text']) for item in batch]\n    title_lengths = [len(item['title']) for item in batch]\n\n    # Pad sequences\n    padded_texts = torch.nn.utils.rnn.pad_sequence([item['text'] for item in batch], padding_value=0)\n    padded_titles = torch.nn.utils.rnn.pad_sequence([item['title'] for item in batch], padding_value=0)\n\n    # Keep raw texts and titles\n    raw_texts = [item['raw_text'] for item in batch]\n    raw_titles = [item['raw_title'] for item in batch]\n\n    return {\n        'text': padded_texts,\n        'title': padded_titles,\n        'text_lengths': torch.tensor(text_lengths),\n        'title_lengths': torch.tensor(title_lengths),\n        'raw_text': raw_texts,\n        'raw_title': raw_titles\n    }\n\n ","metadata":{"id":"FmZW0OTg71XH","trusted":true,"execution":{"iopub.status.busy":"2025-04-15T17:49:25.205950Z","iopub.execute_input":"2025-04-15T17:49:25.206171Z","iopub.status.idle":"2025-04-15T17:49:25.964062Z","shell.execute_reply.started":"2025-04-15T17:49:25.206155Z","shell.execute_reply":"2025-04-15T17:49:25.963450Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"print(\"Loading data...\")\ntrain_df = pd.read_csv('/kaggle/input/wiki-data/train.csv')\nval_df = train_df.sample(n=500, random_state=42)\ntrain_df = train_df.drop(val_df.index)\ntest_df = pd.read_csv('/kaggle/input/wiki-data/test.csv')","metadata":{"id":"DYyDdXKu_mDw","trusted":true,"execution":{"iopub.status.busy":"2025-04-15T17:49:25.964750Z","iopub.execute_input":"2025-04-15T17:49:25.964967Z","iopub.status.idle":"2025-04-15T17:49:31.923439Z","shell.execute_reply.started":"2025-04-15T17:49:25.964949Z","shell.execute_reply":"2025-04-15T17:49:31.922769Z"}},"outputs":[{"name":"stdout","text":"Loading data...\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"print(\"Preprocessing data...\")\ntrain_df['text'] = train_df['text'].apply(lambda x: improved_preprocess(x, lower_case=True, stopword_removal=True))\nval_df['text'] = val_df['text'].apply(lambda x: improved_preprocess(x, lower_case=True, stopword_removal=True))\ntest_df['text'] = test_df['text'].apply(lambda x: improved_preprocess(x, lower_case=True, stopword_removal=True))","metadata":{"id":"28FodxIc_os6","trusted":true,"execution":{"iopub.status.busy":"2025-04-15T17:49:31.924240Z","iopub.execute_input":"2025-04-15T17:49:31.924494Z","iopub.status.idle":"2025-04-15T17:53:43.485815Z","shell.execute_reply.started":"2025-04-15T17:49:31.924473Z","shell.execute_reply":"2025-04-15T17:53:43.485263Z"}},"outputs":[{"name":"stdout","text":"Preprocessing data...\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"train_df['original_title'] = train_df['title']\nval_df['original_title'] = val_df['title']\ntest_df['original_title'] = test_df['title']\ntrain_df['title'] = train_df['title'].apply(lambda x: improved_preprocess(x, lower_case=True, stopword_removal=False))\nval_df['title'] = val_df['title'].apply(lambda x: improved_preprocess(x, lower_case=True, stopword_removal=False))","metadata":{"id":"Nl3JRrIr_rGz","trusted":true,"execution":{"iopub.status.busy":"2025-04-15T17:53:43.486525Z","iopub.execute_input":"2025-04-15T17:53:43.486744Z","iopub.status.idle":"2025-04-15T17:53:44.231411Z","shell.execute_reply.started":"2025-04-15T17:53:43.486726Z","shell.execute_reply":"2025-04-15T17:53:44.230874Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"print(\"Building vocabulary...\")\nall_texts = list(train_df['text']) + list(train_df['title'])\nvocab = build_vocab(all_texts, min_freq_ratio=0.0000007)  # Using very low threshold to prevent ommiting most words","metadata":{"id":"D35XbnXj_xE0","trusted":true,"execution":{"iopub.status.busy":"2025-04-15T17:53:44.233070Z","iopub.execute_input":"2025-04-15T17:53:44.233260Z","iopub.status.idle":"2025-04-15T17:55:58.946095Z","shell.execute_reply.started":"2025-04-15T17:53:44.233245Z","shell.execute_reply":"2025-04-15T17:55:58.945306Z"}},"outputs":[{"name":"stdout","text":"Building vocabulary...\nVocabulary size: 46040\nMin count threshold: 19\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"# Create datasets\ntrain_dataset = WikiTitleDataset(train_df, vocab, max_length_text=512, max_length_title=30)\nval_dataset = WikiTitleDataset(val_df, vocab, max_length_text=512, max_length_title=30)\ntest_dataset = WikiTitleDataset(test_df, vocab, max_length_text=512, max_length_title=30)\n\nbatch_size = 16  # Adjust based on your GPU memory\n\n# Create data loaders\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, collate_fn=collate_fn, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=batch_size, collate_fn=collate_fn)\ntest_loader = DataLoader(test_dataset, batch_size=8, collate_fn=collate_fn)  # Smaller batch for testing\n","metadata":{"id":"vnMIJjkP_0q4","trusted":true,"execution":{"iopub.status.busy":"2025-04-15T17:55:58.947122Z","iopub.execute_input":"2025-04-15T17:55:58.947374Z","iopub.status.idle":"2025-04-15T17:55:58.952614Z","shell.execute_reply.started":"2025-04-15T17:55:58.947355Z","shell.execute_reply":"2025-04-15T17:55:58.951943Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"vocab_size = len(vocab)\nembedding_dim = 300\nhidden_dim = 300\n\n# Path to GloVe embeddings\nglove_path = '/kaggle/input/wiki-datas/glove.6B.300d.txt'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T17:55:58.953672Z","iopub.execute_input":"2025-04-15T17:55:58.953905Z","iopub.status.idle":"2025-04-15T17:55:58.970590Z","shell.execute_reply.started":"2025-04-15T17:55:58.953880Z","shell.execute_reply":"2025-04-15T17:55:58.969994Z"}},"outputs":[],"execution_count":9}]}