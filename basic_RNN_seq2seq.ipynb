{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[]},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11391962,"sourceType":"datasetVersion","datasetId":7134363},{"sourceId":11378511,"sourceType":"datasetVersion","datasetId":7124096}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport re\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nimport numpy as np\nimport random\nimport time\nfrom collections import Counter\nimport spacy\nfrom tqdm import tqdm\nimport os\nimport gc\n\n# Download necessary NLTK resources\nnltk.download('punkt')\nnltk.download('stopwords')\nnltk.download('wordnet')\nnltk.download('punkt_tab')","metadata":{"id":"NYX8c9DS-hGw","trusted":true,"execution":{"iopub.status.busy":"2025-04-15T15:40:55.913809Z","iopub.execute_input":"2025-04-15T15:40:55.914528Z","iopub.status.idle":"2025-04-15T15:41:06.344280Z","shell.execute_reply.started":"2025-04-15T15:40:55.914493Z","shell.execute_reply":"2025-04-15T15:41:06.343714Z"}},"outputs":[{"name":"stderr","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n[nltk_data] Downloading package punkt_tab to /usr/share/nltk_data...\n[nltk_data]   Package punkt_tab is already up-to-date!\n","output_type":"stream"},{"execution_count":1,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":1},{"cell_type":"code","source":"os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"","metadata":{"id":"Xan6_YEi-pFt","trusted":true,"execution":{"iopub.status.busy":"2025-04-15T15:41:06.345619Z","iopub.execute_input":"2025-04-15T15:41:06.346499Z","iopub.status.idle":"2025-04-15T15:41:06.349931Z","shell.execute_reply.started":"2025-04-15T15:41:06.346473Z","shell.execute_reply":"2025-04-15T15:41:06.349150Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"def optimize_memory():\n    \"\"\"Apply memory optimization techniques for Kaggle\"\"\"\n    # Set PyTorch to release memory when no longer needed\n    torch.cuda.empty_cache()\n\n    # Set environment variables for better memory management\n    os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n\n    # Check available GPU memory\n    if torch.cuda.is_available():\n        print(f\"Total GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n        print(f\"Available GPU memory: {torch.cuda.memory_reserved(0) / 1e9:.2f} GB\")\n\noptimize_memory()\n","metadata":{"id":"8EJzMMsWAfXo","trusted":true,"execution":{"iopub.status.busy":"2025-04-15T15:41:06.350716Z","iopub.execute_input":"2025-04-15T15:41:06.351044Z","iopub.status.idle":"2025-04-15T15:41:06.390917Z","shell.execute_reply.started":"2025-04-15T15:41:06.351020Z","shell.execute_reply":"2025-04-15T15:41:06.390416Z"}},"outputs":[{"name":"stdout","text":"Total GPU memory: 15.83 GB\nAvailable GPU memory: 0.00 GB\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"try:\n    from rouge import Rouge\nexcept ImportError:\n    !pip install rouge\n    from rouge import Rouge","metadata":{"id":"rirlDalT-rJx","trusted":true,"execution":{"iopub.status.busy":"2025-04-15T15:41:06.391594Z","iopub.execute_input":"2025-04-15T15:41:06.391829Z","iopub.status.idle":"2025-04-15T15:41:09.888449Z","shell.execute_reply.started":"2025-04-15T15:41:06.391813Z","shell.execute_reply":"2025-04-15T15:41:09.887620Z"}},"outputs":[{"name":"stdout","text":"Collecting rouge\n  Downloading rouge-1.0.1-py3-none-any.whl.metadata (4.1 kB)\nRequirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from rouge) (1.17.0)\nDownloading rouge-1.0.1-py3-none-any.whl (13 kB)\nInstalling collected packages: rouge\nSuccessfully installed rouge-1.0.1\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"SEED = 42\ntorch.manual_seed(SEED)\ntorch.cuda.manual_seed(SEED) if torch.cuda.is_available() else None\nnp.random.seed(SEED)\nrandom.seed(SEED)","metadata":{"id":"ADeg53h5-0NI","trusted":true,"execution":{"iopub.status.busy":"2025-04-15T15:41:09.890441Z","iopub.execute_input":"2025-04-15T15:41:09.890675Z","iopub.status.idle":"2025-04-15T15:41:09.899706Z","shell.execute_reply.started":"2025-04-15T15:41:09.890654Z","shell.execute_reply":"2025-04-15T15:41:09.899032Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# Set device\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using device: {device}\")","metadata":{"id":"NEFI6QlY-2VG","trusted":true,"execution":{"iopub.status.busy":"2025-04-15T15:41:09.900544Z","iopub.execute_input":"2025-04-15T15:41:09.900801Z","iopub.status.idle":"2025-04-15T15:41:09.911896Z","shell.execute_reply.started":"2025-04-15T15:41:09.900779Z","shell.execute_reply":"2025-04-15T15:41:09.911384Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"\n\n# Set seeds for reproducibility\nSEED = 42\ntorch.manual_seed(SEED)\ntorch.cuda.manual_seed(SEED) if torch.cuda.is_available() else None\nnp.random.seed(SEED)\nrandom.seed(SEED)\n\n# Set device\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using device: {device}\")\n\n# Load spaCy for sentence tokenization\ntry:\n    nlp = spacy.load(\"en_core_web_sm\")\nexcept:\n    import subprocess\n    subprocess.call(\"python -m spacy download en_core_web_sm\", shell=True)\n    nlp = spacy.load(\"en_core_web_sm\")\n\n# Initialize NLP tools\nstop_words = set(stopwords.words('english'))\nminimal_stopwords = set(['the', 'and', 'a', 'of', 'to', 'in', 'that', 'it', 'with', 'for', 'on', 'at'])\nlemmatizer = WordNetLemmatizer()\n\ndef improved_preprocess(text, lower_case=True, lemmatize=True, stopword_removal=True):\n    \"\"\"Improved preprocessing function that preserves more contextual information\"\"\"\n    if lower_case:\n        text = text.lower()\n\n    # Remove non-alphanumeric characters except for basic punctuation\n    text = re.sub(r'[^a-zA-Z0-9\\s\\.\\,\\?\\!]', '', text)\n\n    # Tokenize\n    tokens = nltk.word_tokenize(text)\n\n    # Remove stopwords if specified\n    if stopword_removal:\n        tokens = [word for word in tokens if word not in minimal_stopwords]\n\n    # Lemmatize if specified\n    if lemmatize:\n        tokens = [lemmatizer.lemmatize(word) for word in tokens]\n\n    # Join tokens back into text\n    processed_text = \" \".join(tokens)\n    return processed_text\n\ndef tokenize(text):\n    \"\"\"Tokenize text to words\"\"\"\n    return nltk.word_tokenize(text.lower())\n\ndef build_vocab(sentences, min_freq_ratio=0.01):\n    \"\"\"Build vocabulary from sentences with minimum frequency threshold\"\"\"\n    all_tokens = [token for sent in sentences for token in tokenize(sent)]\n    total = len(all_tokens)\n    counter = Counter(all_tokens)\n\n    # Calculate minimum count threshold\n    min_count = max(1, int(total * min_freq_ratio))\n\n    # Initialize vocabulary with special tokens\n    vocab = {\"<pad>\": 0, \"<unk>\": 1, \"<bos>\": 2, \"<eos>\": 3}\n    idx = 4\n\n    # Add words that meet the frequency threshold\n    for word, count in counter.items():\n        if count >= min_count:\n            vocab[word] = idx\n            idx += 1\n\n    print(f\"Vocabulary size: {len(vocab)}\")\n    print(f\"Min count threshold: {min_count}\")\n    return vocab\n\nclass WikiTitleDataset(Dataset):\n    def __init__(self, df, vocab, max_length_text=512, max_length_title=30):\n        self.df = df\n        self.vocab = vocab\n        self.max_length_text = max_length_text\n        self.max_length_title = max_length_title\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        text = self.df.iloc[idx]['text']\n        title = self.df.iloc[idx]['title']\n\n        # Convert text to indices\n        text_tokens = tokenize(text)[:self.max_length_text]\n        text_indices = [self.vocab.get(token, self.vocab['<unk>']) for token in text_tokens]\n\n        # Convert title to indices\n        title_tokens = tokenize(title)[:self.max_length_title-2]  # -2 for <bos> and <eos>\n        title_indices = [self.vocab['<bos>']] + [self.vocab.get(token, self.vocab['<unk>']) for token in title_tokens] + [self.vocab['<eos>']]\n\n        return {\n            'text': torch.tensor(text_indices, dtype=torch.long),\n            'title': torch.tensor(title_indices, dtype=torch.long),\n            'raw_text': text,\n            'raw_title': title\n        }\n\ndef collate_fn(batch):\n    \"\"\"Custom collate function for DataLoader\"\"\"\n    # Sort batch by text length in descending order for packed sequences\n    batch = sorted(batch, key=lambda x: len(x['text']), reverse=True)\n\n    text_lengths = [len(item['text']) for item in batch]\n    title_lengths = [len(item['title']) for item in batch]\n\n    # Pad sequences\n    padded_texts = torch.nn.utils.rnn.pad_sequence([item['text'] for item in batch], padding_value=0)\n    padded_titles = torch.nn.utils.rnn.pad_sequence([item['title'] for item in batch], padding_value=0)\n\n    # Keep raw texts and titles\n    raw_texts = [item['raw_text'] for item in batch]\n    raw_titles = [item['raw_title'] for item in batch]\n\n    return {\n        'text': padded_texts,\n        'title': padded_titles,\n        'text_lengths': torch.tensor(text_lengths),\n        'title_lengths': torch.tensor(title_lengths),\n        'raw_text': raw_texts,\n        'raw_title': raw_titles\n    }\n\nclass EncoderRNN(nn.Module):\n    def __init__(self, vocab_size, embedding_dim, hidden_dim, dropout=0.3):\n        super(EncoderRNN, self).__init__()\n        self.hidden_dim = hidden_dim\n        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n        self.gru = nn.GRU(embedding_dim, hidden_dim, bidirectional=True, batch_first=False)\n        self.dropout = nn.Dropout(dropout)\n        self.fc = nn.Linear(hidden_dim * 2, hidden_dim)\n\n    def forward(self, x, lengths=None):\n        \"\"\"\n        Args:\n            x: Input sequence tensor [seq_len, batch_size]\n            lengths: Length of each sequence in the batch\n        Returns:\n            outputs: GRU outputs [seq_len, batch_size, hidden_dim * 2]\n            hidden: Final hidden state [1, batch_size, hidden_dim]\n        \"\"\"\n        embedded = self.dropout(self.embedding(x))\n\n        if lengths is not None:\n            packed = torch.nn.utils.rnn.pack_padded_sequence(embedded, lengths.cpu())\n            outputs, hidden = self.gru(packed)\n            outputs, _ = torch.nn.utils.rnn.pad_packed_sequence(outputs)\n        else:\n            outputs, hidden = self.gru(embedded)\n\n        # Concatenate bidirectional hidden states\n        hidden = torch.cat((hidden[0], hidden[1]), dim=1)\n        hidden = torch.tanh(self.fc(hidden))\n        hidden = hidden.unsqueeze(0)\n\n        return outputs, hidden\n\n    def load_embeddings(self, pretrained_embeddings):\n        \"\"\"Load pretrained word embeddings\"\"\"\n        self.embedding.weight.data.copy_(pretrained_embeddings)\n        print(\"Loaded pretrained embeddings successfully!\")\n\nclass HierEncoderRNN(nn.Module):\n    def __init__(self, vocab_size, embedding_dim, hidden_dim, dropout=0.3):\n        super(HierEncoderRNN, self).__init__()\n        self.hidden_dim = hidden_dim\n        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n        self.word_gru = nn.GRU(embedding_dim, hidden_dim, bidirectional=True, batch_first=False)\n        self.sent_gru = nn.GRU(hidden_dim * 2, hidden_dim, bidirectional=True, batch_first=True)\n        self.dropout = nn.Dropout(dropout)\n        self.fc = nn.Linear(hidden_dim * 2, hidden_dim)\n\n    def forward(self, x, lengths=None, sentence_boundaries=None):\n        \"\"\"\n        Args:\n            x: Input sequence tensor [seq_len, batch_size]\n            lengths: Length of each sequence in the batch\n            sentence_boundaries: List of indices where sentences end for each batch item\n                Format: List of lists, where each inner list contains sentence end indices\n        Returns:\n            outputs: GRU outputs [seq_len, batch_size, hidden_dim * 2]\n            hidden: Final hidden state [1, batch_size, hidden_dim]\n        \"\"\"\n        batch_size = x.shape[1]\n        embedded = self.dropout(self.embedding(x))\n\n        if lengths is None:\n            seq_len = x.size(0)\n            lengths = torch.full((batch_size,), seq_len, device=x.device)\n\n        # Process at word level\n        packed = torch.nn.utils.rnn.pack_padded_sequence(embedded, lengths.cpu(), enforce_sorted=False)\n        word_outputs, word_hidden = self.word_gru(packed)\n        word_outputs, _ = torch.nn.utils.rnn.pad_packed_sequence(word_outputs)\n\n        # If sentence boundaries not provided, create artificial ones\n        if sentence_boundaries is None:\n            sentence_boundaries = [list(range(20, min(length.item(), 500), 20)) for length in lengths]\n            for i, length in enumerate(lengths):\n                if length.item() not in sentence_boundaries[i]:\n                    sentence_boundaries[i].append(length.item())\n\n        # Process at sentence level\n        sent_level_outputs = []\n        for batch_idx in range(batch_size):\n            sent_ends = sentence_boundaries[batch_idx]\n            if not sent_ends:\n                sent_avg = torch.mean(word_outputs[:lengths[batch_idx], batch_idx, :], dim=0)\n                sent_level_outputs.append(sent_avg.unsqueeze(0))\n            else:\n                sent_reprs = []\n                prev_end = 0\n                for end in sent_ends:\n                    if end > prev_end:  # Ensure we don't process empty sentences\n                        sent_repr = torch.mean(word_outputs[prev_end:end, batch_idx, :], dim=0)\n                        sent_reprs.append(sent_repr)\n                    prev_end = end\n\n                if sent_reprs:\n                    batch_sent_reprs = torch.stack(sent_reprs)\n                    sent_level_outputs.append(batch_sent_reprs)\n                else:\n                    sent_avg = torch.mean(word_outputs[:lengths[batch_idx], batch_idx, :], dim=0)\n                    sent_level_outputs.append(sent_avg.unsqueeze(0))\n\n        # Pad sentence representations to same length\n        max_sent_count = max(output.size(0) for output in sent_level_outputs)\n        sent_padded = []\n        for output in sent_level_outputs:\n            if output.size(0) < max_sent_count:\n                padding = torch.zeros(max_sent_count - output.size(0), output.size(1), device=x.device)\n                sent_padded.append(torch.cat([output, padding], dim=0))\n            else:\n                sent_padded.append(output)\n\n        sent_batch = torch.stack(sent_padded, dim=1)\n        sent_lengths = torch.tensor([output.size(0) for output in sent_level_outputs], device=x.device)\n\n        # Process through sentence-level GRU\n        sent_outputs, sent_hidden = self.sent_gru(sent_batch.transpose(0, 1))\n\n        # Combine bidirectional hidden states\n        hidden = torch.cat((sent_hidden[0], sent_hidden[1]), dim=1)\n        hidden = torch.tanh(self.fc(hidden))\n        hidden = hidden.unsqueeze(0)\n\n        return word_outputs, hidden\n\n    def load_embeddings(self, pretrained_embeddings):\n        \"\"\"Load pretrained word embeddings\"\"\"\n        self.embedding.weight.data.copy_(pretrained_embeddings)\n        print(\"Loaded pretrained embeddings successfully!\")\n\nclass DecoderRNN(nn.Module):\n    def __init__(self, vocab_size, embedding_dim, hidden_dim, dropout=0.3):\n        super(DecoderRNN, self).__init__()\n        self.hidden_dim = hidden_dim\n        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n        self.gru = nn.GRU(embedding_dim, hidden_dim, batch_first=False)\n        self.fc = nn.Linear(hidden_dim, vocab_size)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x, hidden):\n        \"\"\"\n        Args:\n            x: Input token tensor [1, batch_size]\n            hidden: Previous hidden state [1, batch_size, hidden_dim]\n        Returns:\n            outputs: Token probabilities [batch_size, vocab_size]\n            hidden: Updated hidden state [1, batch_size, hidden_dim]\n        \"\"\"\n        embedded = self.dropout(self.embedding(x))\n        output, hidden = self.gru(embedded, hidden)\n        output = self.fc(output.squeeze(0))  # [batch_size, vocab_size]\n        output = F.log_softmax(output, dim=1)  # [batch_size, vocab_size]\n        return output, hidden\n\nclass Decoder2RNN(nn.Module):\n    def __init__(self, vocab_size, embedding_dim, hidden_dim, dropout=0.3):\n        super(Decoder2RNN, self).__init__()\n        self.hidden_dim = hidden_dim\n        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n        self.gru1 = nn.GRU(embedding_dim, hidden_dim, batch_first=False)\n        self.gru2 = nn.GRU(hidden_dim, hidden_dim, batch_first=False)\n        self.fc = nn.Linear(hidden_dim, vocab_size)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x, hidden):\n        \"\"\"\n        Args:\n            x: Input token tensor [1, batch_size]\n            hidden: Previous hidden state [1, batch_size, hidden_dim]\n        Returns:\n            output: Token probabilities [batch_size, vocab_size]\n            hidden: Updated hidden state from second GRU [1, batch_size, hidden_dim]\n        \"\"\"\n        embedded = self.dropout(self.embedding(x))\n        output1, hidden1 = self.gru1(embedded, hidden)\n        output1 = self.dropout(output1)\n        output2, hidden2 = self.gru2(output1, hidden)\n        output = self.fc(output2.squeeze(0))\n        output = F.log_softmax(output, dim=1)\n        return output, hidden2\n\nclass Seq2seqRNN(nn.Module):\n    def __init__(self, vocab_size, embedding_dim, hidden_dim, vocab=None,\n                 encoder_type='basic', decoder_type='basic', use_pretrained=False,\n                 embeddings_path=None):\n        super().__init__()\n        self.vocab_size = vocab_size\n        self.embedding_dim = embedding_dim\n        self.hidden_dim = hidden_dim\n        self.vocab = vocab\n        self.max_length = 30\n\n        # Initialize encoder based on type\n        if encoder_type == 'hierarchical':\n            self.encoder = HierEncoderRNN(vocab_size, embedding_dim, hidden_dim)\n        else:\n            self.encoder = EncoderRNN(vocab_size, embedding_dim, hidden_dim)\n\n        # Initialize decoder based on type\n        if decoder_type == 'dual':\n            self.decoder = Decoder2RNN(vocab_size, embedding_dim, hidden_dim)\n        else:  # Default to basic decoder\n            self.decoder = DecoderRNN(vocab_size, embedding_dim, hidden_dim)\n\n        # Load pretrained embeddings if specified\n        if use_pretrained and embeddings_path:\n            self._load_pretrained_embeddings(embeddings_path)\n\n    def _load_pretrained_embeddings(self, embeddings_path):\n        \"\"\"Load GloVe embeddings from file\"\"\"\n        print(f\"Loading pretrained embeddings from {embeddings_path}...\")\n        weights_matrix = torch.FloatTensor(self.vocab_size, self.embedding_dim).normal_(0, 0.1)\n\n        word2vec = {}\n        with open(embeddings_path, 'r', encoding='utf-8') as f:\n            for line in f:\n                values = line.split()\n                word = values[0]\n                vector = torch.FloatTensor([float(val) for val in values[1:]])\n                word2vec[word] = vector\n\n        words_found = 0\n        for word, idx in self.vocab.items():\n            if word in word2vec:\n                weights_matrix[idx] = word2vec[word]\n                words_found += 1\n\n        print(f\"Found embeddings for {words_found}/{self.vocab_size} words\")\n        self.encoder.load_embeddings(weights_matrix)\n\n    def forward(self, src, target=None, teacher_forcing_ratio=0.5, search_method='greedy', beam_size=3, lengths=None):\n        \"\"\"\n        Args:\n            src: Source sequence [seq_len, batch_size]\n            target: Target sequence [seq_len, batch_size] (for training)\n            teacher_forcing_ratio: Probability of using teacher forcing\n            search_method: 'greedy' or 'beam' for decoding strategy\n            beam_size: Number of beams to use in beam search\n            lengths: Sequence lengths for packed sequences\n        \"\"\"\n        batch_size = src.shape[1]\n\n        # Encode input sequence\n        encoder_outputs, hidden = self.encoder(src, lengths=lengths)\n\n        # If target is provided (training mode)\n        if target is not None:\n            target_len = target.shape[0]\n            outputs = torch.zeros(target_len, batch_size, self.vocab_size).to(src.device)\n\n            # First input to the decoder is the <bos> token\n            decoder_input = target[0, :].unsqueeze(0)  # [1, batch_size]\n\n            # Teacher forcing: Feed the target as the next input\n            for t in range(1, target_len):\n                decoder_output, hidden = self.decoder(decoder_input, hidden)\n                outputs[t] = decoder_output\n\n                # Decide if we use teacher forcing or not\n                use_teacher_force = random.random() < teacher_forcing_ratio\n\n                # Get the highest predicted token\n                top1 = decoder_output.argmax(1)\n\n                # Use teacher forcing: use actual target token as next input\n                # Otherwise: use predicted token\n                decoder_input = target[t].unsqueeze(0) if use_teacher_force else top1.unsqueeze(0)\n\n            return outputs\n\n        # If no target is provided (inference mode)\n        else:\n            if search_method == 'beam':\n                return self._beam_search_decode(hidden, batch_size, beam_size)\n            else:\n                return self._greedy_decode(hidden, batch_size)\n\n    def _greedy_decode(self, hidden, batch_size):\n        \"\"\"Greedy decoding for inference\"\"\"\n        # First input to the decoder is the <bos> token\n        decoder_input = torch.tensor([[self.vocab['<bos>']] * batch_size], device=hidden.device)\n\n        outputs = []\n        finished = [False] * batch_size\n\n        for t in range(self.max_length):\n            decoder_output, hidden = self.decoder(decoder_input, hidden)\n\n            # Get the highest predicted token\n            top1 = decoder_output.argmax(1)\n            outputs.append(top1)\n\n            # Next input is the predicted token\n            decoder_input = top1.unsqueeze(0)\n\n            # Check if any sequences have reached <eos>\n            for i in range(batch_size):\n                if top1[i] == self.vocab['<eos>']:\n                    finished[i] = True\n\n            # If all sequences have reached <eos>, stop decoding\n            if all(finished):\n                break\n\n        # If we have no outputs (very unlikely), add at least one token\n        if not outputs:\n            # Add a token (e.g., <unk>) to prevent empty output\n            dummy_output = torch.tensor([self.vocab['<unk>']] * batch_size, device=hidden.device)\n            outputs.append(dummy_output)\n\n        return torch.stack(outputs)\n    \n    def _beam_search_decode(self, hidden, batch_size, beam_size=3):\n        \"\"\"\n        Beam search decoding for inference\n        Args:\n            hidden: Initial hidden state from encoder [1, batch_size, hidden_dim]\n            batch_size: Batch size\n            beam_size: Number of beams to track\n        Returns:\n            outputs: Tensor of shape [seq_len, batch_size] with best sequences\n        \"\"\"\n        # Process each batch item separately\n        all_best_sequences = []\n        device = hidden.device\n    \n        for b in range(batch_size):\n            # Get hidden state for this batch item\n            batch_hidden = hidden[:, b:b+1, :].clone()\n            \n            # First token is always <bos>\n            start_token = torch.tensor([[self.vocab['<bos>']]], device=device)\n            \n            # Initialize beam with just the start token\n            beams = [\n                {\n                    'sequence': [self.vocab['<bos>']],  # Use list for sequences\n                    'score': 0.0,\n                    'hidden': batch_hidden.clone(),\n                    'finished': False\n                }\n                for _ in range(beam_size)\n            ]\n            \n            # For the first timestep, all beams have the same start token and hidden state\n            decoder_output, new_hidden = self.decoder(start_token, batch_hidden)\n            \n            # Get top-k tokens for the first step\n            topk_probs, topk_indices = decoder_output.squeeze(0).topk(beam_size)\n            \n            # Initialize beams with top-k tokens from first step\n            for i in range(beam_size):\n                beams[i]['sequence'] = [self.vocab['<bos>'], topk_indices[i].item()]\n                beams[i]['score'] = topk_probs[i].item()\n                beams[i]['hidden'] = new_hidden.clone()\n                beams[i]['finished'] = topk_indices[i].item() == self.vocab['<eos>']\n                \n            # For each additional timestep\n            for t in range(1, self.max_length - 1):  # -1 because we already did one step\n                # Check if all beams are finished\n                if all(beam['finished'] for beam in beams):\n                    break\n                    \n                # Collect candidates from all beams\n                candidates = []\n                \n                # Process active beams\n                for beam_idx, beam in enumerate(beams):\n                    if beam['finished']:\n                        # Keep finished beams in the candidates\n                        candidates.append({\n                            'sequence': beam['sequence'],\n                            'score': beam['score'],\n                            'hidden': beam['hidden'],\n                            'finished': True,\n                            'parent_beam': beam_idx\n                        })\n                    else:\n                        # Continue this active beam\n                        last_token = torch.tensor([[beam['sequence'][-1]]], device=device)\n                        decoder_output, new_hidden = self.decoder(last_token, beam['hidden'])\n                        \n                        # Get top-k tokens for this beam\n                        topk_probs, topk_indices = decoder_output.squeeze(0).topk(beam_size)\n                        \n                        # Calculate scores\n                        for i in range(beam_size):\n                            token = topk_indices[i].item()\n                            new_score = beam['score'] + topk_probs[i].item()\n                            \n                            candidates.append({\n                                'sequence': beam['sequence'] + [token],\n                                'score': new_score,\n                                'hidden': new_hidden.clone(),\n                                'finished': token == self.vocab['<eos>'],\n                                'parent_beam': beam_idx\n                            })\n                \n                # Select top beams based on score\n                candidates.sort(key=lambda x: x['score'], reverse=True)\n                beams = candidates[:beam_size]\n            \n            # Select the best beam\n            best_beam = max(beams, key=lambda x: x['score'])\n            best_sequence = best_beam['sequence']\n            \n            # Remove start token and end token if present\n            if best_sequence[0] == self.vocab['<bos>']:\n                best_sequence = best_sequence[1:]\n            if best_sequence and best_sequence[-1] == self.vocab['<eos>']:\n                best_sequence = best_sequence[:-1]\n                \n            # Convert to tensor\n            best_tensor = torch.tensor(best_sequence, device=device)\n            all_best_sequences.append(best_tensor)\n        \n        # Pad sequences to same length\n        max_len = max(seq.size(0) for seq in all_best_sequences) if all_best_sequences else 1\n        padded_sequences = []\n        \n        for seq in all_best_sequences:\n            if seq.size(0) < max_len:\n                padding = torch.zeros(max_len - seq.size(0), device=device).long()\n                padded_seq = torch.cat([seq, padding], dim=0)\n            else:\n                padded_seq = seq\n            padded_sequences.append(padded_seq)\n        \n        # Stack and transpose to get [seq_len, batch_size]\n        result = torch.stack(padded_sequences).transpose(0, 1)\n        return result","metadata":{"id":"FmZW0OTg71XH","trusted":true,"execution":{"iopub.status.busy":"2025-04-15T15:41:09.912756Z","iopub.execute_input":"2025-04-15T15:41:09.912990Z","iopub.status.idle":"2025-04-15T15:41:10.680624Z","shell.execute_reply.started":"2025-04-15T15:41:09.912975Z","shell.execute_reply":"2025-04-15T15:41:10.680060Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"import gc\nimport torch\nfrom torch.cuda.amp import autocast, GradScaler\n\n# Initialize gradient scaler for mixed precision training\nscaler = GradScaler()\n\ndef train_epoch(model, dataloader, optimizer, criterion, clip=1.0, teacher_forcing_ratio=0.5):\n    \"\"\"Train the model for one epoch using mixed precision\"\"\"\n    model.train()\n    epoch_loss = 0\n\n    for batch in tqdm(dataloader, desc=\"Training\"):\n        # Move data to device\n        src = batch['text'].to(device)\n        trg = batch['title'].to(device)\n        text_lengths = batch['text_lengths']\n\n        # Zero gradients\n        optimizer.zero_grad()\n\n        # Use mixed precision for memory efficiency\n        with autocast():\n            output = model(src, trg, teacher_forcing_ratio, lengths=text_lengths)\n\n            # Make sure shapes are correct\n            output = output[1:].reshape(-1, output.shape[-1])  # skip <bos> for output\n            trg = trg[1:].reshape(-1)  # skip <bos> for target\n\n            loss = criterion(output, trg)\n\n        # Scaled backward pass\n        scaler.scale(loss).backward()\n\n        # Clip gradients\n        scaler.unscale_(optimizer)  # Unscale before clipping\n        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n\n        # Step with scaler\n        scaler.step(optimizer)\n        scaler.update()\n\n        epoch_loss += loss.item()\n\n        # Free up unused memory\n        torch.cuda.empty_cache()\n        gc.collect()\n\n    return epoch_loss / len(dataloader)\n\ndef evaluate(model, dataloader, criterion):\n    \"\"\"Evaluate with proper handling of padding\"\"\"\n    model.eval()\n    epoch_loss = 0\n    batch_count = 0\n\n    with torch.no_grad():\n        for batch in tqdm(dataloader, desc=\"Evaluating\"):\n            src = batch['text'].to(device)\n            trg = batch['title'].to(device)\n            text_lengths = batch['text_lengths']\n\n            # No teacher forcing\n            output = model(src, trg, teacher_forcing_ratio=0.0, lengths=text_lengths)\n\n            # Skip <bos> token in both output and trg\n            output = output[1:].reshape(-1, output.shape[-1])\n            trg = trg[1:].reshape(-1)\n\n            # Calculate loss\n            loss = criterion(output, trg)\n            epoch_loss += loss.item()\n            batch_count += 1\n\n            # Free memory\n            torch.cuda.empty_cache()\n            gc.collect()\n\n    torch.cuda.empty_cache()\n    return epoch_loss / batch_count\n\ndef generate_titles(model, dataloader, vocab, search_method='greedy', beam_size=3):\n    \"\"\"Generate titles for the test set\"\"\"\n    model.eval()\n    idx_to_word = {idx: word for word, idx in vocab.items()}\n    generated_titles = []\n    reference_titles = []\n\n    with torch.no_grad():\n        for batch in tqdm(dataloader, desc=\"Generating titles\"):\n            src = batch['text'].to(device)\n            text_lengths = batch['text_lengths']\n\n            output = model(src, target=None, teacher_forcing_ratio=0,\n                          search_method=search_method, beam_size=beam_size if search_method == 'beam' else None,\n                          lengths=text_lengths)\n\n            for i in range(output.shape[1]):\n                pred_title = []\n                for j in range(output.shape[0]):\n                    idx = output[j, i].item()\n                    if idx == 0 or idx == vocab['<eos>']:  # Stop at padding or EOS token\n                        break\n                    if idx in idx_to_word and idx != vocab['<bos>']:  # Skip BOS token\n                        pred_title.append(idx_to_word[idx])\n\n                # Format the title properly\n                if pred_title:\n                    # Capitalize first word\n                    if pred_title[0]:\n                        pred_title[0] = pred_title[0].capitalize()\n\n                    # Capitalize proper nouns and other important words\n                    for j in range(1, len(pred_title)):\n                        if pred_title[j] not in minimal_stopwords and len(pred_title[j]) > 2:\n                            pred_title[j] = pred_title[j].capitalize()\n                else:\n                    pred_title = [\"Untitled\"]  # Default title if nothing generated\n\n                generated_titles.append(' '.join(pred_title))\n                reference_titles.append(batch['raw_title'][i])\n\n            # Free memory\n            torch.cuda.empty_cache()\n            gc.collect()\n\n    return generated_titles, reference_titles\n","metadata":{"id":"Rw-ogk3S-VOm","trusted":true,"execution":{"iopub.status.busy":"2025-04-15T15:41:10.681287Z","iopub.execute_input":"2025-04-15T15:41:10.681484Z","iopub.status.idle":"2025-04-15T15:41:10.694146Z","shell.execute_reply.started":"2025-04-15T15:41:10.681466Z","shell.execute_reply":"2025-04-15T15:41:10.693395Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_31/3932397477.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  scaler = GradScaler()\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"def train_model(model, train_loader, val_loader, optimizer, criterion, scheduler=None,\n               n_epochs=15, clip=1.0, teacher_forcing_ratio=0.5, patience=5, model_name='basic'):\n    \"\"\"Train the model with early stopping and learning rate scheduler\"\"\"\n    best_val_loss = float('inf')\n    patience_counter = 0\n    train_losses = []\n    val_losses = []\n\n    for epoch in range(n_epochs):\n        start_time = time.time()\n\n        # Gradually decrease teacher forcing ratio\n        current_tf_ratio = max(0.1, teacher_forcing_ratio * (1.0 - epoch/n_epochs))\n\n        train_loss = train_epoch(model, train_loader, optimizer, criterion, clip, current_tf_ratio)\n        val_loss = evaluate(model, val_loader, criterion)\n\n        if scheduler is not None:\n            scheduler.step(val_loss)\n\n        train_losses.append(train_loss)\n        val_losses.append(val_loss)\n\n        end_time = time.time()\n        epoch_mins, epoch_secs = divmod(end_time - start_time, 60)\n\n        print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs:.0f}s')\n        print(f'\\tTrain Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}')\n        print(f'\\tTeacher forcing ratio: {current_tf_ratio:.2f}')\n        print(f'\\tCurrent LR: {optimizer.param_groups[0][\"lr\"]:.6f}')\n\n        # Save model if validation loss improves\n        if val_loss < best_val_loss:\n            best_val_loss = val_loss\n            torch.save(model.state_dict(), f'best_model_{model_name}.pt')\n            patience_counter = 0\n            print(\"\\tSaved new best model!\")\n        else:\n            patience_counter += 1\n            print(f\"\\tNo improvement: patience {patience_counter}/{patience}\")\n\n        # Early stopping\n        if patience_counter >= patience:\n            print(\"Early stopping triggered!\")\n            break\n\n        # Free up memory\n        torch.cuda.empty_cache()\n        gc.collect()\n\n    return train_losses, val_losses\n\ndef calculate_rouge(generated_titles, reference_titles):\n    \"\"\"Calculate ROUGE scores between generated and reference titles\"\"\"\n    rouge = Rouge()\n    valid_pairs = []\n\n    for gen, ref in zip(generated_titles, reference_titles):\n        # ROUGE requires non-empty strings\n        if len(gen.strip()) == 0:\n            gen = \"untitled\"\n        if len(ref.strip()) == 0:\n            ref = \"untitled\"\n        valid_pairs.append((gen, ref))\n\n    hyps, refs = zip(*valid_pairs)\n\n    try:\n        scores = rouge.get_scores(hyps, refs, avg=True)\n        return scores\n    except Exception as e:\n        print(f\"Error calculating ROUGE scores: {e}\")\n        # Handle specific ROUGE errors by fixing problematic pairs\n        fixed_pairs = []\n        for gen, ref in valid_pairs:\n            # Ensure minimum length for ROUGE calculation\n            if len(gen.split()) < 1:\n                gen = \"untitled\"\n            if len(ref.split()) < 1:\n                ref = \"untitled\"\n            fixed_pairs.append((gen, ref))\n\n        if fixed_pairs:\n            hyps, refs = zip(*fixed_pairs)\n            try:\n                scores = rouge.get_scores(hyps, refs, avg=True)\n                return scores\n            except:\n                pass\n\n        # Return default scores in case of error\n        return {\n            'rouge-1': {'f': 0.0, 'p': 0.0, 'r': 0.0},\n            'rouge-2': {'f': 0.0, 'p': 0.0, 'r': 0.0},\n            'rouge-l': {'f': 0.0, 'p': 0.0, 'r': 0.0}\n        }\n\ndef load_glove_embeddings(embeddings_path, vocab, embedding_dim=300):\n    \"\"\"\n    Load GloVe embeddings for the vocabulary with memory optimization\n    Args:\n        embeddings_path: Path to the GloVe embeddings file\n        vocab: Dictionary mapping words to indices\n        embedding_dim: Dimension of embeddings\n    Returns:\n        weights_matrix: Tensor of shape [vocab_size, embedding_dim]\n    \"\"\"\n    print(f\"Loading GloVe embeddings from {embeddings_path}...\")\n    weights_matrix = torch.FloatTensor(len(vocab), embedding_dim).normal_(0, 0.1)\n\n    # Process the GloVe file in chunks to save memory\n    word2vec = {}\n    words_in_vocab = set(vocab.keys())\n\n    with open(embeddings_path, 'r', encoding='utf-8') as f:\n        for line in tqdm(f, desc=\"Reading GloVe file\"):\n            values = line.split()\n            word = values[0]\n\n            # Only store embeddings for words in our vocabulary\n            if word in words_in_vocab:\n                vector = torch.FloatTensor([float(val) for val in values[1:]])\n                word2vec[word] = vector\n\n            # Periodically clear memory\n            if len(word2vec) % 50000 == 0:\n                gc.collect()\n\n    words_found = 0\n    for word, idx in tqdm(vocab.items(), desc=\"Mapping to vocabulary\"):\n        if word in word2vec:\n            weights_matrix[idx] = word2vec[word]\n            words_found += 1\n\n    print(f\"Found embeddings for {words_found}/{len(vocab)} words\")\n\n    # Clear memory\n    del word2vec\n    gc.collect()\n\n    return weights_matrix\n\n","metadata":{"id":"K73LlXfK-ZSN","trusted":true,"execution":{"iopub.status.busy":"2025-04-15T15:41:10.695057Z","iopub.execute_input":"2025-04-15T15:41:10.695464Z","iopub.status.idle":"2025-04-15T15:41:10.717645Z","shell.execute_reply.started":"2025-04-15T15:41:10.695440Z","shell.execute_reply":"2025-04-15T15:41:10.716927Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"print(\"Loading data...\")\ntrain_df = pd.read_csv('/kaggle/input/wiki-dataset/train.csv')\nval_df = train_df.sample(n=500, random_state=42)\ntrain_df = train_df.drop(val_df.index)\ntest_df = pd.read_csv('/kaggle/input/wiki-dataset/test.csv')","metadata":{"id":"DYyDdXKu_mDw","trusted":true,"execution":{"iopub.status.busy":"2025-04-15T15:41:58.309734Z","iopub.execute_input":"2025-04-15T15:41:58.310038Z","iopub.status.idle":"2025-04-15T15:42:03.244995Z","shell.execute_reply.started":"2025-04-15T15:41:58.310017Z","shell.execute_reply":"2025-04-15T15:42:03.244467Z"}},"outputs":[{"name":"stdout","text":"Loading data...\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"print(\"Preprocessing data...\")\ntrain_df['text'] = train_df['text'].apply(lambda x: improved_preprocess(x, lower_case=True, stopword_removal=True))\nval_df['text'] = val_df['text'].apply(lambda x: improved_preprocess(x, lower_case=True, stopword_removal=True))\ntest_df['text'] = test_df['text'].apply(lambda x: improved_preprocess(x, lower_case=True, stopword_removal=True))","metadata":{"id":"28FodxIc_os6","trusted":true,"execution":{"iopub.status.busy":"2025-04-15T15:42:16.282572Z","iopub.execute_input":"2025-04-15T15:42:16.282817Z","iopub.status.idle":"2025-04-15T15:46:29.685652Z","shell.execute_reply.started":"2025-04-15T15:42:16.282801Z","shell.execute_reply":"2025-04-15T15:46:29.685036Z"}},"outputs":[{"name":"stdout","text":"Preprocessing data...\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"train_df['original_title'] = train_df['title']\nval_df['original_title'] = val_df['title']\ntest_df['original_title'] = test_df['title']\ntrain_df['title'] = train_df['title'].apply(lambda x: improved_preprocess(x, lower_case=True, stopword_removal=False))\nval_df['title'] = val_df['title'].apply(lambda x: improved_preprocess(x, lower_case=True, stopword_removal=False))","metadata":{"id":"Nl3JRrIr_rGz","trusted":true,"execution":{"iopub.status.busy":"2025-04-15T15:46:29.686664Z","iopub.execute_input":"2025-04-15T15:46:29.686895Z","iopub.status.idle":"2025-04-15T15:46:30.471175Z","shell.execute_reply.started":"2025-04-15T15:46:29.686878Z","shell.execute_reply":"2025-04-15T15:46:30.470401Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"print(\"Building vocabulary...\")\nall_texts = list(train_df['text']) + list(train_df['title'])\nvocab = build_vocab(all_texts, min_freq_ratio=0.0000007)  # Using 1% threshold as specified","metadata":{"id":"D35XbnXj_xE0","trusted":true,"execution":{"iopub.status.busy":"2025-04-15T15:46:30.471953Z","iopub.execute_input":"2025-04-15T15:46:30.472223Z","iopub.status.idle":"2025-04-15T15:48:45.575388Z","shell.execute_reply.started":"2025-04-15T15:46:30.472207Z","shell.execute_reply":"2025-04-15T15:48:45.574754Z"}},"outputs":[{"name":"stdout","text":"Building vocabulary...\nVocabulary size: 46040\nMin count threshold: 19\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"# Create datasets\ntrain_dataset = WikiTitleDataset(train_df, vocab, max_length_text=512, max_length_title=30)\nval_dataset = WikiTitleDataset(val_df, vocab, max_length_text=512, max_length_title=30)\ntest_dataset = WikiTitleDataset(test_df, vocab, max_length_text=512, max_length_title=30)\n\nbatch_size = 16  # Adjust based on your GPU memory\n\n# Create data loaders\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, collate_fn=collate_fn, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=batch_size, collate_fn=collate_fn)\ntest_loader = DataLoader(test_dataset, batch_size=8, collate_fn=collate_fn)  # Smaller batch for testing\n","metadata":{"id":"vnMIJjkP_0q4","trusted":true,"execution":{"iopub.status.busy":"2025-04-15T15:48:45.577227Z","iopub.execute_input":"2025-04-15T15:48:45.577848Z","iopub.status.idle":"2025-04-15T15:48:45.582938Z","shell.execute_reply.started":"2025-04-15T15:48:45.577820Z","shell.execute_reply":"2025-04-15T15:48:45.582280Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"vocab_size = len(vocab)\nembedding_dim = 300\nhidden_dim = 300\n\n# Path to GloVe embeddings\nglove_path = '/kaggle/input/wiki-dataset/glove.6B.300d.txt'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T15:48:45.583824Z","iopub.execute_input":"2025-04-15T15:48:45.584069Z","iopub.status.idle":"2025-04-15T15:48:45.604309Z","shell.execute_reply.started":"2025-04-15T15:48:45.584050Z","shell.execute_reply":"2025-04-15T15:48:45.603581Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"# Set batch size for basic model\ntorch.cuda.empty_cache()\nbatch_size = 32\n\n# Create datasets\ntrain_dataset = WikiTitleDataset(train_df, vocab, max_length_text=512, max_length_title=10)\nval_dataset = WikiTitleDataset(val_df, vocab, max_length_text=512, max_length_title=10)\ntest_dataset = WikiTitleDataset(test_df, vocab, max_length_text=512, max_length_title=10)\n\n# Create data loaders\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, collate_fn=collate_fn, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=batch_size, collate_fn=collate_fn)\ntest_loader = DataLoader(test_dataset, batch_size=8, collate_fn=collate_fn)\n\n# Train BASIC model\nprint(f\"\\n{'='*50}\\nTraining basic model\\n{'='*50}\")\n\nmodel_basic = Seq2seqRNN(\n    vocab_size=vocab_size,\n    embedding_dim=embedding_dim,\n    hidden_dim=hidden_dim,\n    vocab=vocab,\n    encoder_type='basic',\n    decoder_type='basic',\n    use_pretrained=False,\n    embeddings_path=None\n).to(device)\n\ntorch.cuda.empty_cache()\ngc.collect()\n\noptimizer = torch.optim.Adam(model_basic.parameters(), lr=0.001, weight_decay=1e-5)\nscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2, verbose=True)\ncriterion = nn.CrossEntropyLoss(ignore_index=0)\n\ntrain_losses, val_losses = train_model(\n    model_basic, train_loader, val_loader, optimizer, criterion,\n    scheduler=scheduler, n_epochs=10, clip=1.0,\n    teacher_forcing_ratio=0.7, patience=3,\n    model_name='basic'\n)\n\nmodel_basic.load_state_dict(torch.load('best_model_basic.pt'))\n\ngenerated_titles, reference_titles = generate_titles(\n    model_basic, test_loader, vocab, search_method='greedy'\n)\n\nprint(\"\\nCalculating ROUGE scores...\")\nrouge_scores = calculate_rouge(generated_titles, reference_titles)\n\nprint(\"\\nROUGE Scores:\")\nprint(f\"ROUGE-1: {rouge_scores['rouge-1']['f']:.4f}\")\nprint(f\"ROUGE-2: {rouge_scores['rouge-2']['f']:.4f}\")\nprint(f\"ROUGE-L: {rouge_scores['rouge-l']['f']:.4f}\")\n\nprint(\"\\nExample predictions:\")\nfor i in range(min(5, len(generated_titles))):\n    print(f\"Reference: {reference_titles[i]}\")\n    print(f\"Generated: {generated_titles[i]}\")\n    print(\"---\")\n\ndel model_basic\ntorch.cuda.empty_cache()\ngc.collect()\n","metadata":{"id":"82mnAF7X_339","trusted":true,"execution":{"iopub.status.busy":"2025-04-15T17:22:41.077584Z","iopub.execute_input":"2025-04-15T17:22:41.077871Z","iopub.status.idle":"2025-04-15T18:07:20.064678Z","shell.execute_reply.started":"2025-04-15T17:22:41.077847Z","shell.execute_reply":"2025-04-15T18:07:20.063944Z"}},"outputs":[{"name":"stdout","text":"\n==================================================\nTraining basic model\n==================================================\n","output_type":"stream"},{"name":"stderr","text":"Training:   0%|          | 0/419 [00:00<?, ?it/s]/tmp/ipykernel_31/3932397477.py:23: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with autocast():\nTraining: 100%|██████████| 419/419 [04:41<00:00,  1.49it/s]\nEvaluating: 100%|██████████| 16/16 [00:09<00:00,  1.60it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 01 | Time: 4.0m 52s\n\tTrain Loss: 5.5365 | Val Loss: 4.3067\n\tTeacher forcing ratio: 0.70\n\tCurrent LR: 0.001000\n\tSaved new best model!\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 419/419 [04:43<00:00,  1.48it/s]\nEvaluating: 100%|██████████| 16/16 [00:09<00:00,  1.61it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 02 | Time: 4.0m 54s\n\tTrain Loss: 3.9983 | Val Loss: 4.0656\n\tTeacher forcing ratio: 0.63\n\tCurrent LR: 0.001000\n\tSaved new best model!\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 419/419 [04:44<00:00,  1.47it/s]\nEvaluating: 100%|██████████| 16/16 [00:10<00:00,  1.53it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 03 | Time: 4.0m 55s\n\tTrain Loss: 3.2199 | Val Loss: 3.9263\n\tTeacher forcing ratio: 0.56\n\tCurrent LR: 0.001000\n\tSaved new best model!\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 419/419 [04:57<00:00,  1.41it/s]\nEvaluating: 100%|██████████| 16/16 [00:10<00:00,  1.58it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 04 | Time: 5.0m 8s\n\tTrain Loss: 2.5238 | Val Loss: 3.8256\n\tTeacher forcing ratio: 0.49\n\tCurrent LR: 0.001000\n\tSaved new best model!\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 419/419 [04:46<00:00,  1.46it/s]\nEvaluating: 100%|██████████| 16/16 [00:10<00:00,  1.57it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 05 | Time: 4.0m 57s\n\tTrain Loss: 1.8814 | Val Loss: 3.8367\n\tTeacher forcing ratio: 0.42\n\tCurrent LR: 0.001000\n\tNo improvement: patience 1/3\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 419/419 [04:43<00:00,  1.48it/s]\nEvaluating: 100%|██████████| 16/16 [00:09<00:00,  1.61it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 06 | Time: 4.0m 54s\n\tTrain Loss: 1.3110 | Val Loss: 3.8251\n\tTeacher forcing ratio: 0.35\n\tCurrent LR: 0.001000\n\tSaved new best model!\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 419/419 [04:41<00:00,  1.49it/s]\nEvaluating: 100%|██████████| 16/16 [00:10<00:00,  1.60it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 07 | Time: 4.0m 52s\n\tTrain Loss: 0.8534 | Val Loss: 3.9088\n\tTeacher forcing ratio: 0.28\n\tCurrent LR: 0.001000\n\tNo improvement: patience 1/3\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 419/419 [04:47<00:00,  1.46it/s]\nEvaluating: 100%|██████████| 16/16 [00:10<00:00,  1.57it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 08 | Time: 4.0m 58s\n\tTrain Loss: 0.5301 | Val Loss: 3.9039\n\tTeacher forcing ratio: 0.21\n\tCurrent LR: 0.001000\n\tNo improvement: patience 2/3\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 419/419 [04:51<00:00,  1.44it/s]\nEvaluating: 100%|██████████| 16/16 [00:10<00:00,  1.56it/s]\n/tmp/ipykernel_31/2672295238.py:43: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model_basic.load_state_dict(torch.load('best_model_basic.pt'))\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 09 | Time: 5.0m 1s\n\tTrain Loss: 0.3545 | Val Loss: 3.9319\n\tTeacher forcing ratio: 0.14\n\tCurrent LR: 0.000500\n\tNo improvement: patience 3/3\nEarly stopping triggered!\n","output_type":"stream"},{"name":"stderr","text":"Generating titles: 100%|██████████| 13/13 [00:05<00:00,  2.54it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nCalculating ROUGE scores...\n\nROUGE Scores:\nROUGE-1: 0.2393\nROUGE-2: 0.0535\nROUGE-L: 0.2393\n\nExample predictions:\nReference: Weyburn\nGenerated: <unk>\n---\nReference: Catholic High School, Singapore\nGenerated: Cyfair High High School\n---\nReference: Minnesota Golden Gophers\nGenerated: La Fighting Fighting\n---\nReference: List of people from Louisiana\nGenerated: List of People From Georgia\n---\nReference: FC Shakhtar Donetsk\nGenerated: Fc Dynamo Moscow\n---\n","output_type":"stream"},{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"768"},"metadata":{}}],"execution_count":22},{"cell_type":"code","source":"batch_size = 16\n\n# Re-create data loaders with smaller batch size\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, collate_fn=collate_fn, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=batch_size, collate_fn=collate_fn)\ntest_loader = DataLoader(test_dataset, batch_size=8, collate_fn=collate_fn)\n\n# Train ALL IMPROVEMENTS model\nprint(f\"\\n{'='*50}\\nTraining all_improvements model\\n{'='*50}\")\n\nmodel_all = Seq2seqRNN(\n    vocab_size=vocab_size,\n    embedding_dim=embedding_dim,\n    hidden_dim=hidden_dim,\n    vocab=vocab,\n    encoder_type='hierarchical',\n    decoder_type='dual',\n    use_pretrained=True,\n    embeddings_path=glove_path\n).to(device)\n\ntorch.cuda.empty_cache()\ngc.collect()\n\n# Update batch size to handle complex model\n\n\noptimizer = torch.optim.Adam(model_all.parameters(), lr=0.001, weight_decay=1e-5)\nscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2, verbose=True)\ncriterion = nn.CrossEntropyLoss(ignore_index=0)\n\ntrain_losses, val_losses = train_model(\n    model_all, train_loader, val_loader, optimizer, criterion,\n    scheduler=scheduler, n_epochs=10, clip=1.0,\n    teacher_forcing_ratio=0.7, patience=3,\n    model_name='all_improvements'\n)\n\n\nmodel_all.load_state_dict(torch.load('best_model_all_improvements.pt'))\n\ngenerated_titles, reference_titles = generate_titles(\n    model_all, test_loader, vocab, search_method='beam', beam_size=3\n)\n\nprint(\"\\nCalculating ROUGE scores...\")\nrouge_scores = calculate_rouge(generated_titles, reference_titles)\n\nprint(\"\\nROUGE Scores:\")\nprint(f\"ROUGE-1: {rouge_scores['rouge-1']['f']:.4f}\")\nprint(f\"ROUGE-2: {rouge_scores['rouge-2']['f']:.4f}\")\nprint(f\"ROUGE-L: {rouge_scores['rouge-l']['f']:.4f}\")\n\nprint(\"\\nExample predictions:\")\nfor i in range(min(5, len(generated_titles))):\n    print(f\"Reference: {reference_titles[i]}\")\n    print(f\"Generated: {generated_titles[i]}\")\n    print(\"---\")\n\ndel model_all\ntorch.cuda.empty_cache()\ngc.collect()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T15:52:42.003445Z","iopub.execute_input":"2025-04-15T15:52:42.004206Z","iopub.status.idle":"2025-04-15T17:17:18.616707Z","shell.execute_reply.started":"2025-04-15T15:52:42.004180Z","shell.execute_reply":"2025-04-15T17:17:18.616044Z"}},"outputs":[{"name":"stdout","text":"\n==================================================\nTraining all_improvements model\n==================================================\nLoading pretrained embeddings from /kaggle/input/wiki-dataset/glove.6B.300d.txt...\nFound embeddings for 41488/46040 words\nLoaded pretrained embeddings successfully!\n","output_type":"stream"},{"name":"stderr","text":"Training:   0%|          | 0/837 [00:00<?, ?it/s]/tmp/ipykernel_31/3932397477.py:23: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with autocast():\nTraining: 100%|██████████| 837/837 [08:04<00:00,  1.73it/s]\nEvaluating: 100%|██████████| 32/32 [00:15<00:00,  2.12it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 01 | Time: 8.0m 19s\n\tTrain Loss: 6.0074 | Val Loss: 5.3107\n\tTeacher forcing ratio: 0.70\n\tCurrent LR: 0.001000\n\tSaved new best model!\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 837/837 [08:04<00:00,  1.73it/s]\nEvaluating: 100%|██████████| 32/32 [00:15<00:00,  2.12it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 02 | Time: 8.0m 19s\n\tTrain Loss: 4.7503 | Val Loss: 4.8399\n\tTeacher forcing ratio: 0.63\n\tCurrent LR: 0.001000\n\tSaved new best model!\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 837/837 [08:05<00:00,  1.72it/s]\nEvaluating: 100%|██████████| 32/32 [00:15<00:00,  2.05it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 03 | Time: 8.0m 21s\n\tTrain Loss: 4.1926 | Val Loss: 4.5152\n\tTeacher forcing ratio: 0.56\n\tCurrent LR: 0.001000\n\tSaved new best model!\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 837/837 [08:07<00:00,  1.72it/s]\nEvaluating: 100%|██████████| 32/32 [00:16<00:00,  2.00it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 04 | Time: 8.0m 24s\n\tTrain Loss: 3.7012 | Val Loss: 4.4633\n\tTeacher forcing ratio: 0.49\n\tCurrent LR: 0.001000\n\tSaved new best model!\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 837/837 [08:23<00:00,  1.66it/s]\nEvaluating: 100%|██████████| 32/32 [00:15<00:00,  2.02it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 05 | Time: 8.0m 39s\n\tTrain Loss: 3.2546 | Val Loss: 4.3864\n\tTeacher forcing ratio: 0.42\n\tCurrent LR: 0.001000\n\tSaved new best model!\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 837/837 [08:22<00:00,  1.67it/s]\nEvaluating: 100%|██████████| 32/32 [00:15<00:00,  2.12it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 06 | Time: 8.0m 37s\n\tTrain Loss: 2.8633 | Val Loss: 4.3496\n\tTeacher forcing ratio: 0.35\n\tCurrent LR: 0.001000\n\tSaved new best model!\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 837/837 [08:00<00:00,  1.74it/s]\nEvaluating: 100%|██████████| 32/32 [00:15<00:00,  2.13it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 09 | Time: 8.0m 16s\n\tTrain Loss: 1.9086 | Val Loss: 4.3983\n\tTeacher forcing ratio: 0.14\n\tCurrent LR: 0.001000\n\tNo improvement: patience 1/3\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 837/837 [08:01<00:00,  1.74it/s]\nEvaluating: 100%|██████████| 32/32 [00:14<00:00,  2.15it/s]\n/tmp/ipykernel_31/2750013711.py:40: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model_all.load_state_dict(torch.load('best_model_all_improvements.pt'))\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 10 | Time: 8.0m 17s\n\tTrain Loss: 1.6394 | Val Loss: 4.4103\n\tTeacher forcing ratio: 0.10\n\tCurrent LR: 0.001000\n\tNo improvement: patience 2/3\n","output_type":"stream"},{"name":"stderr","text":"Generating titles: 100%|██████████| 13/13 [00:05<00:00,  2.26it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nCalculating ROUGE scores...\n\nROUGE Scores:\nROUGE-1: 0.1291\nROUGE-2: 0.0202\nROUGE-L: 0.1291\n\nExample predictions:\nReference: Weyburn\nGenerated: <unk>\n---\nReference: Catholic High School, Singapore\nGenerated: St. High School\n---\nReference: Minnesota Golden Gophers\nGenerated: Washington Stadium\n---\nReference: List of people from Louisiana\nGenerated: List of People From Utah\n---\nReference: FC Shakhtar Donetsk\nGenerated: Fc Basel\n---\n","output_type":"stream"},{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"709"},"metadata":{}}],"execution_count":21},{"cell_type":"code","source":"model_all = Seq2seqRNN(\n    vocab_size=vocab_size,\n    embedding_dim=embedding_dim,\n    hidden_dim=hidden_dim,\n    vocab=vocab,\n    encoder_type='hierarchical',\n    decoder_type='dual',\n    use_pretrained=False,\n    embeddings_path=glove_path\n).to(device)\nmodel_all.load_state_dict(torch.load('best_model_all_improvements.pt'))\n\ngenerated_titles, reference_titles = generate_titles(\n    model_all, test_loader, vocab, search_method='greedy', beam_size=3\n)\n\nprint(\"\\nCalculating ROUGE scores greedy + decoder2 + hierarchical encoder...\")\nrouge_scores = calculate_rouge(generated_titles, reference_titles)\n\nprint(\"\\nROUGE Scores:\")\nprint(f\"ROUGE-1: {rouge_scores['rouge-1']['f']:.4f}\")\nprint(f\"ROUGE-2: {rouge_scores['rouge-2']['f']:.4f}\")\nprint(f\"ROUGE-L: {rouge_scores['rouge-l']['f']:.4f}\")\n\nprint(\"\\nExample predictions:\")\nfor i in range(min(5, len(generated_titles))):\n    print(f\"Reference: {reference_titles[i]}\")\n    print(f\"Generated: {generated_titles[i]}\")\n    print(\"---\")\n\ndel model_all\ntorch.cuda.empty_cache()\ngc.collect()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T18:08:52.033760Z","iopub.execute_input":"2025-04-15T18:08:52.034028Z","iopub.status.idle":"2025-04-15T18:08:58.407137Z","shell.execute_reply.started":"2025-04-15T18:08:52.034008Z","shell.execute_reply":"2025-04-15T18:08:58.406562Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_31/2751949104.py:11: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model_all.load_state_dict(torch.load('best_model_all_improvements.pt'))\nGenerating titles: 100%|██████████| 13/13 [00:05<00:00,  2.40it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nCalculating ROUGE scores greedy + decoder2 + hierarchical encoder...\n\nROUGE Scores:\nROUGE-1: 0.1330\nROUGE-2: 0.0238\nROUGE-L: 0.1330\n\nExample predictions:\nReference: Weyburn\nGenerated: <unk>\n---\nReference: Catholic High School, Singapore\nGenerated: St. High School\n---\nReference: Minnesota Golden Gophers\nGenerated: Washington Stadium\n---\nReference: List of people from Louisiana\nGenerated: List of People From Utah\n---\nReference: FC Shakhtar Donetsk\nGenerated: Fc Basel\n---\n","output_type":"stream"},{"execution_count":26,"output_type":"execute_result","data":{"text/plain":"705"},"metadata":{}}],"execution_count":26},{"cell_type":"code","source":"model_basic = Seq2seqRNN(\n    vocab_size=vocab_size,\n    embedding_dim=embedding_dim,\n    hidden_dim=hidden_dim,\n    vocab=vocab,\n    encoder_type='basic',\n    decoder_type='basic',\n    use_pretrained=False,\n    embeddings_path=None\n).to(device)\n\nmodel_basic.load_state_dict(torch.load('best_model_basic.pt'))\n\ngenerated_titles, reference_titles = generate_titles(\n    model_basic, test_loader, vocab, search_method='beam'\n)\n\nprint(\"\\nCalculating ROUGE scores...beam + basic rnn\")\nrouge_scores = calculate_rouge(generated_titles, reference_titles)\n\nprint(\"\\nROUGE Scores:\")\nprint(f\"ROUGE-1: {rouge_scores['rouge-1']['f']:.4f}\")\nprint(f\"ROUGE-2: {rouge_scores['rouge-2']['f']:.4f}\")\nprint(f\"ROUGE-L: {rouge_scores['rouge-l']['f']:.4f}\")\n\nprint(\"\\nExample predictions:\")\nfor i in range(min(5, len(generated_titles))):\n    print(f\"Reference: {reference_titles[i]}\")\n    print(f\"Generated: {generated_titles[i]}\")\n    print(\"---\")\n\ndel model_basic\ntorch.cuda.empty_cache()\ngc.collect()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T18:08:10.547593Z","iopub.execute_input":"2025-04-15T18:08:10.548190Z","iopub.status.idle":"2025-04-15T18:08:17.018013Z","shell.execute_reply.started":"2025-04-15T18:08:10.548169Z","shell.execute_reply":"2025-04-15T18:08:17.017481Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_31/2808898642.py:12: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model_basic.load_state_dict(torch.load('best_model_basic.pt'))\nGenerating titles: 100%|██████████| 13/13 [00:05<00:00,  2.30it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nCalculating ROUGE scores...beam + basic rnn\n\nROUGE Scores:\nROUGE-1: 0.2395\nROUGE-2: 0.0477\nROUGE-L: 0.2395\n\nExample predictions:\nReference: Weyburn\nGenerated: <unk>\n---\nReference: Catholic High School, Singapore\nGenerated: Cyfair High High School\n---\nReference: Minnesota Golden Gophers\nGenerated: La Fighting\n---\nReference: List of people from Louisiana\nGenerated: List of People People\n---\nReference: FC Shakhtar Donetsk\nGenerated: Fc Dynamo Moscow\n---\n","output_type":"stream"},{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"695"},"metadata":{}}],"execution_count":24},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}
